{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = (\n",
    "    \"http://biostat.mc.vanderbilt.edu/\"\n",
    "    \"wiki/pub/Main/DataSets/titanic3.xls\"\n",
    ")\n",
    "df = pd.read_excel(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    709\n",
       "1    323\n",
       "2    277\n",
       "Name: pclass, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Classes\n",
    "\n",
    "If you are classifying data, and the classes are not relatively balanced in size, the bias toward more popular classes can carry over into your model. \n",
    "\n",
    "For example, if you have 1 positive case and 99 negative cases, you can get 99% accuracy simply by classifying everthing as negative. There are various options for dealing with *imbalanced classes*.\n",
    "\n",
    "## Use a different metric\n",
    "\n",
    "\n",
    "One hint is to use a measure other than accuracy (AUC is a good choice) for calibrating models. Precision and recall are also better options when the target sizes are different. However, there are other options to consider.\n",
    "\n",
    "## Tree-based Algorithms and Ensembles\n",
    "\n",
    "Three-based models may perform better depending on the distribution of the smaller class. If they tend to be clustered, they can be classified easily.\n",
    "\n",
    "Enseble methods can futher aid in pulling out the minority classes. Bagging and boosting are options found in tree models like random forests and Extreme Gradient Boosting (XGBoost). \n",
    "\n",
    "## Penalize Models\n",
    "\n",
    "Many scikit-learn classification models support the `class_weight` parameter. Setting this to 'balanced' will attempt to regularize minority classes and incentivize themodel to classify them correctly. \n",
    "\n",
    "Alternatively, you can use a grid search and specify the weight options by passing in a dictionary mapping class to weight (give a higher weight to smaller classes)\n",
    "\n",
    "The XGBoost library has the `max_delta_step` parameter, which can be set from 1 to 10 to make the update step more conservative. It also has the `scale_pos_wieght` parameter that sets the ration of negative to positve samples (for binary classes). The `eval_metric` should be set to `auc` rather than tthe default.\n",
    "\n",
    "The KNN model has a`weights` parameter that can bias neightbors that are closer. If the minority class samples are close together, setting this parameter to `distance` may improve performance. \n",
    "\n",
    "## Upsampling Minority\n",
    "\n",
    "you can upsample the minority class in a couple of ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "mask = df['survived'] == 1\n",
    "surv_df = df[mask]\n",
    "death_df = df[~mask]\n",
    "df_upsample = resample(surv_df, replace=True, \n",
    "                       n_samples=len(death_df), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    809\n",
       "1    500\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([death_df, df_upsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    809\n",
       "0    809\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling Majority\n",
    "\n",
    "Another method to balance classes is to downsample majority classes. \n",
    "\n",
    "**Don't replace when downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsample = resample(death_df, \n",
    "                         replace=False, n_samples=len(surv_df), \n",
    "                         random_state = 42)\n",
    "# Don't replace when downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([surv_df, df_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ds4b)",
   "language": "python",
   "name": "pycharm-d7086c1b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
